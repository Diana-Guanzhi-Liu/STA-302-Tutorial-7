---
title: "STA 302 Tutorial 7"
format: pdf
author: Diana Liu
thanks: "Code and data are available at: https://github.com/Diana-Guanzhi-Liu/STA-302-Tutorial-7. Peer Review by Hannah Yu: https://github.com/Diana-Guanzhi-Liu/STA-302-Tutorial-7/issues/1" 
date: 22 February 2024
date-format: long
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

observations <- 1000
mean <- 1
sd <- 1

library(dplyr)
library(ggpubr)
library(kableExtra)
```

# Introduction
In this exercise, we simulate errors in data collection and cleaning using @CiteR and discuss how they can effect the estimated mean of a dataset in the data section and potential ways to catch errors. The data set with errors has mean of 0.99, standard deviation of 1, and error of 0.03 (@tbl-error). Its probability density function and histogram seems to have binomial distribution with a peak between 0 and 1 and another between 1 and 2 (@fig-dataset-graphs) (@fig-dataset-histogram). Mean and standard deviation from the data set with errors do still suggest that true mean is greater than zero. 

In general, errors can be detected by plotting the data set. How the graph differs from expectations can alert us to the fact that there could have been errors in the collection and cleaning of data. If we know that certain errors can occur, we can build functions that detects them, for example checking the frequency of negative observations.

# Data
First we simulate a data set of 900 observations with normal distribution, mean of 1 and standard deviation of 1. In situation one, the last 100 observations of the data set has been replaced with the first 100. This is simulated by storing the first 100 observations in a variable then adding them to the end of the data set. Now the data set has 1000 observations and the first 100 are identical to the last 100 (@tbl-situation-1).
```{r}
#| echo: false
#| warning: false
#| message: false
#| label: tbl-situation-1
#| tbl-cap: "Comparing the first the first 10 observations to the 901 to 910 observations, we find that they are identical due to over-writing by the instrument"
#1. Unknown to us, the instrument has a mistake in it, which means that it has a maximum memory of 900 observations, and begins over-writing at that point, so the final 100 observations are actually a repeat of the first 100.

set.seed(123)
simulation <- tibble(draws = rnorm(n = observations - 100, mean = mean, sd = sd))
first_100 <- head(simulation, 100)
simulation <- add_row(simulation, first_100)

head <- kable(head(simulation, 10), col.names = "head")
tail <- kable(simulation[901:910, "draws"], col.names = "tail")
knitr::kable(list(head, tail))
```
In situation two half of the negative numbers in the data set are changed to positives. First we count the number of negative numbers and divide it by two to get the amount of negatives that has to be changed. Then we iterate through the data set and change negatives to positives by multiplying by negative one until half of negatives have been changed (@tbl-situation-2). If we scroll through the data set, negative numbers only being to appear after 500 observations, which is consistent with turning the first half of them positive.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: tbl-situation-2
#| tbl-cap: "Comparing the first 10 observations after the change to before, notice that the negative observation has been turned positive"
#2. We employ a research assistant to clean and prepare the data set. During the process of doing this, unknown to us, they accidentally change half of the negative draws to be positive.

half_of_negative_draws <- round(sum(simulation$draws < 0, na.rm=TRUE)/2, 0)
negative_draws <- 0

for(i in 1:nrow(simulation)){
  if(negative_draws < half_of_negative_draws){
    if(simulation[i, "draws"] < 0){
      simulation[i, "draws"] <- simulation[i, "draws"] * -1
      negative_draws <- negative_draws + 1
    }
  }
}
head2 <- kable(head(simulation, 10), col.names = "head_no_negatives")
knitr::kable(list(head, head2))
```

In situation three, the decimal place of any value between 1 and 1.1 is shifter to the left by one digit. To do this we iterate through the data set and check if the observation is between 1 and 1.1, if true, multiply it by 0.1 (@tbl-situation-3).

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: tbl-situation-3
#| tbl-cap: "Comparing the first 10 observations from after this change to when the data set was first simulated, decimals for observations between 1 and 1.1 have been shifted to the left "
# 3. They additionally, accidentally, change the decimal place on any value between 1 and 1.1, so that, for instance 1 becomes 0.1, and 1.1 would become 0.11.

for(i in 1:nrow(simulation)){
  if(simulation[i, "draws"] <= 1.1 & simulation[i, "draws"] >= 1){
    simulation[i, "draws"] <- simulation[i, "draws"] * 0.1
  }
}
head3 <- kable(head(simulation, 10), col.names = "head_smaller_decimals")
knitr::kable(list(head, head3))
```

# Discussion
Now the mean, standard deviation, and standard error from the simulated data set with errors can be compared to a simulated data set without errors. We know that the true mean and standard deviation would be one, and the error free data set has mean of 1.03 and standard deviation of 1 which are quite close (@tbl-error-free). Its density function looks to be perfectly normally distributed with mean of one (@fig-dataset-graphs). The data set with errors has mean of 0.99, standard deviation of 1, and error of 0.03 (@tbl-error). It seems to have binomial distribution with a peak between 0 and 1 and another between 1 and 2 (@fig-dataset-graphs).  Mean and standard deviation from the data set with errors do still suggest that true mean is greater than zero.

In general, these errors can be detected by plotting the data set in a probability density graph. We expect the plot to be normally distributed centered around one, and we have enough draws according to the Law of Large Numbers to get the correct shape [@CiteWasserman]. Because the graph is not the correct shape, this alerts us to the fact that there could have been errors in the collection and cleaning of data.

## Situation 1
The change in situation 1 caused the first 100 observations to be the same as the last 100 so there are only 900 unique observations compared to 1000 for the error free data set. This is unlikely to significantly effect the mean & standard deviation and distribution because the observations are randomly generated and the number of observations were not significantly effected.

This mistake is difficult to flag during analysis as it is difficult to tell if a pattern that exists in a data set is due to a mistake or not. If we are aware of the mistake in the instrument, we can manually review the data or build a function that compares the first 100 observations to the last 100 to detect if over writing occurred. 

## Situation 2

Situation 2 caused half of negative observations to become positive. This will increase the mean of the data set with errors. This is likely what creates the peak around 1.25 in the density plot and histogram (@fig-dataset-graphs) (@fig-dataset-histogram) as negative values on the left half of the graph are turned positive and moved to the right half. We expect observations to be normally distributed around 1 with standard deviation of 1 so most observations are between 0 and 2. This means that situation 2 is unlikely to effect standard deviation as any negative numbers that are converted to positive numbers influence standard deviation by the same amount. 

This error can be detected by checking the frequency of negative values. We would expect the simulation to generate negative values that are evenly spaced throughout the observations. With a function that detects the frequency of negative observations of just by manually scrolling through the data set, we will be able to flag this error and similar errors. 

## Situation 3
Situation 3 decreased the mean by decreasing observations from 1 to 1.1 by a factor of ten. this is likely why there is a peak around 0.75 as those values are shifted left. This likely also decreases standard deviation as effects of observations that are closer to the mean are smaller than that of observations that are larger.

This error can be detected in a similar way as negative numbers by looking at the distribution of all numbers and noticing that there are no observations between 1 and 1.1 or seeing that the histogram bin that should contain observations between 1 and 1.1 has far less observations than what we expect (@fig-dataset-histogram).

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: tbl-error-free
#| tbl-cap: "Error free data set has mean of 1.03, standard deviation of 1, and standard error of 0.03"
simulation_cleaned <- tibble(draws = rnorm(n = observations, mean = mean, sd = sd))

true_mean <-
  sum(simulation_cleaned$draws) / nrow(simulation_cleaned)

simulation_cleaned <-
  simulation_cleaned |>
  mutate(diff_square = (draws - true_mean) ^ 2)

true_standard_deviation <-
  sqrt(sum(simulation_cleaned$diff_square) / (nrow(simulation_cleaned) - 1))

true_standard_error <-
  true_standard_deviation / sqrt(nrow(simulation_cleaned))

kable(
  tibble(mean = true_mean,
         sd = true_standard_deviation,
         se = true_standard_error),
  col.names = c(
    "True mean",
    "True standard deviation",
    "True standard error"
  ),
  digits = 2,
  align = c("l", "r", "r"),
  booktabs = TRUE,
  linesep = ""
  )
  
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: tbl-error
#| tbl-cap: "Data set with errors has mean of 0.11, standard deviation of 0.09, and standard error of 0"
estimated_mean <-
  sum(simulation$draws) / nrow(simulation)

simulation <-
  simulation |>
  mutate(diff_square = (draws - estimated_mean) ^ 2)

estimated_standard_deviation <-
  sqrt(sum(simulation$diff_square) / (nrow(simulation) - 1))

estimated_standard_error <-
  estimated_standard_deviation / sqrt(nrow(simulation))

kable(
  tibble(mean = estimated_mean,
         sd = estimated_standard_deviation,
         se = estimated_standard_error),
  col.names = c(
    "Estimated mean",
    "Estimated standard deviation",
    "Estimated standard error"
  ),
  digits = 2,
  align = c("l", "r", "r"),
  booktabs = TRUE,
  linesep = ""
  )
```


```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-dataset-graphs
#| fig-cap: "Error free data is normally distributed and has mean of one. Data with errors seems to be binomially distributed with one peak at 0.75 and another at 1.25."
cleaned_plot <-
  simulation_cleaned |>
  ggplot(aes(x = draws)) +
  geom_density() +
  ggtitle("Error Free Data") +
  theme_minimal() 

simulated_plot <-
  simulation |>
  ggplot(aes(x = draws)) +
  geom_density() +
  ggtitle("Data with Errors") +
  theme_minimal() 

ggarrange(cleaned_plot, simulated_plot, ncol = 2)
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-dataset-histogram
#| fig-cap: "Error free data is normally distributed with mean of one. Data with errors is binomially distributed with a peak between 0 and 1 and another between 1 and 2"

cleaned_histogram <-
  simulation_cleaned |>
  ggplot(aes(x = draws)) +
  geom_histogram(binwidth = .2) +
  ggtitle("Error Free Data") +
  theme_minimal() 

simulated_histogram <-
  simulation |>
  ggplot(aes(x = draws)) +
  geom_histogram(binwidth = .2) +
  ggtitle("Data with Errors") +
  theme_minimal() 

ggarrange(cleaned_histogram, simulated_histogram, ncol = 2)
```
\newpage
# References

